\centering
\includegraphics[width=.8\textwidth]{framework/Logo_HS_Coburg}

\begin{Large}
  Hochschule für angewandte Wissenschaften Coburg\\
  Fakultät Elektrotechnik und Informatik\par
\end{Large}
\vspace{1.5cm}

\Large{Studiengang: \Studiengang}
\vspace{1.5cm}

\Large{\DocumentType}
\vspace{1cm}

\Huge{\Titel}
\vspace{2cm}

\huge{\Autorenname}
\vspace{2cm}

\Large{Abgabe der Arbeit: \Abgabe}

\Large{Betreut durch:}

\Large{\Betreuer, Hochschule Coburg}

\newpage
\begin{abstract}
\raggedright
Ziel dieser Arbeit war es, eine Künstliche Intelligenz, für das Brettspiel \textit{"Ganz schön clever"}, zu entwickeln und den Entwicklungs- sowie Trainingsprozess zu evaluieren.
Diese Künstliche Intelligenz wurde erfolgreich entwickelt und kann das Brettspiel \textit{"Ganz schön clever"} gut spielen. Gut bedeutet, dass sie in der Lage ist im Durchschnitt eine hohe Punktezahl zu erzielen. Zunächst wurde dafür ein Prototyp entwickelt, welcher dann schrittweise erweitert wurde, um die gesamte Komplexität des Spiels zu erfassen und um es gut spielen zu können.

Das Brettspiel \textit{"Ganz schön clever"} ist ein Würfelspiel, welches eine hohe Komplexität aufweist. Diese kommt vor allem durch die vielen Aktionsmöglichkeiten des Spielers und die multiplen Zusammenhängen innerhalb des Belohnungssystems zustande. Außerdem weist es eine hohe Stochastizität auf, welche die Komplexität weiter erhöht.


Die Spielumgebung und die Künstliche Intelligenz mussten vollständig implementiert werden. Dies geschah mithilfe von Bibliotheken wie Stable Baselines 3 und Gymnasium. Aus zeitlichen Gründen wurde lediglich die Ein-Spieler-Variante des Spiels implementiert. Für die Implementierung der Künstlichen Intelligenz wurde Deep Reinforcement Learning verwendet. Das Neuronale Netz der Künstlichen Intelligenz ist ein Multilayer Perceptron mit sieben Schichten mit jeweils 1024 Neuronen, die eine ReLU-Aktivierungsfunktion verwenden.

Während der Implementierungsphase kam es zunächst zu einem Problem, bei dem die Künstliche Intelligenz es nur schwer selbstständig schaffte, gültige Aktionen zu wählen, welche nicht zum Abbruch des Spiels geführt haben. Das Vorgehen, bei dem das Spiel nach Wahl einer ungültigen Aktion abgebrochen wurde, wurde nach einigen Tagen verworfen, da es der Künstlichen Intelligenz das Lernen erschwerte, weil die meisten Spiele bereits nach wenigen Schritten endeten und die Künstliche Intelligenz so wenig relevante Daten sammeln konnte. Außerdem war die Bewertung der Performanz ebenfalls, durch die vielen Spielabbrüche, erschwert. Daraufhin wurde zunächst ein Verfahren implementiert, bei dem das Spiel bei ungültigen Aktionen nicht sofort abgebrochen, sondern negativ belohnt wurde. Diese Vorgehen stellte sich als gut geeignet heraus, wurde aber schnell von einem Vorgehen, welches eine Aktionsmaske verwendet abgelöst. Im folgenden wurde eine Aktionsmaske implementiert, welche gewährleistet, dass nur gültige Aktionen gewählt werden können. Somit musste die Künstliche Intelligenz nicht mehr selbstständig lernen, welche Aktionen gültig sind. Die Performanz stieg mit diesem Vorgehen von vorher durchschnittlich ungefähr 60 Prozent der Maximalpunktzahl auf 75 Prozent.

Zudem ergaben sich Schwierigkeiten bei der Erweiterung des Runden-Systems. Fast die gesamte Spielumgebung musste, an das neue Runden-System, angepasst werden. Dieser Aufwand wäre voraussichtlich deutlich geringer ausgefallen, wenn das Runden-System von Anfang an vollständig implementiert worden wäre.

Das finale Training der Künstlichen Intelligenz erfolgte in von drei Phasen. Zunächst wurde die KI in 2.220.000 Trainingsschritten vortrainiert. Anschließend wurde sie in 1.110.000 Trainingsschritten, mit einem verringerten Entropie-Koeffizienten, was einer geringeren Exploration entspricht, nachtrainiert, um das gelernte Verhalten zu verfestigen. Dieses Training erfolgte daraufhin mit der doppelten Zeit weitere zwei Male. Das Ergebnis des Trainings war eine Künstliche Intelligenz, welche durchschnittlich 207 Punkte im Spiel erzielt. Im Vergleich dazu erzielten menschliche Spieler in einem Test im Durchschnitt lediglich 160 Punkte. Für die durchschnittlichen Punktezahlen, wurden die erreichten Punkte einer abgeschlossenen Spielumgebung abgespeichert, anschließend aufaddiert und durch ihre Anzahl geteilt.
\end{abstract}

\newpage