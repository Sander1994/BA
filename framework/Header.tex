\centering
\includegraphics[width=.8\textwidth]{framework/Logo_HS_Coburg}

\begin{Large}
  Hochschule für angewandte Wissenschaften Coburg\\
  Fakultät Elektrotechnik und Informatik\par
\end{Large}
\vspace{1.5cm}

\Large{Studiengang: \Studiengang}
\vspace{1.5cm}

\Large{\DocumentType}
\vspace{1cm}

\Huge{\Titel}
\vspace{2cm}

\huge{\Autorenname}
\vspace{2cm}

\Large{Abgabe der Arbeit: \Abgabe}

\Large{Betreut durch:}

\Large{\Betreuer, Hochschule Coburg}

\newpage
\begin{abstract}
\raggedright
Ziel dieser Arbeit war es eine Künstliche Intelligenz, für das Brettspiel "Ganz schön clever", zu entwickeln und den Entwicklungs- sowie Trainingsprozess zu evaluieren.
Diese Künstliche Intelligenz wurde erfolgreich entwickelt und kann das Brettspiel "Ganz schön clever" gut spielen. Zunächst wurde dafür ein Prototyp entwickelt, welcher dann schrittweise erweitert worden ist, um die gesamte Komplexität des Spiels zu erfassen und um es gut spielen zu können.

Das Brettspiel "Ganz schön clever" ist ein Würfelspiel, welches eine hohe Komplexität aufweist. Diese kommt vor allem durch die vielen Aktionsmöglichkeiten des Spielers und die multiplen zusammenhängen innerhalb des Belohnungssystems zustande. Außerdem weist es eine hohe Stochastizität auf, welche die Komplexität weiter erhöht.


Die Spielumgebung und die Künstliche Intelligenz mussten vollständig implementiert werden. Dies geschah mithilfe von Bibliotheken wie Stable-Baselines3 und Gymnasium. Aus zeitlichen Gründen wurde lediglich die Ein-Spieler-Variante des Spiels implementiert. Für die Implementierung der Künstlichen Intelligenz wurde Deep Reinforcement Learning verwendet. Das Neuronale Netz der Künstlichen Intelligenz ist ein Mulilayer Perceptron mit sieben Schichten mit jeweils 1024 Neuronen, die eine ReLU-Aktivierungsfunktion verwenden.

Während der Implementierungsphase kam es zunächst zu einem Problem, bei dem die Künstliche Intelligenz es nur schwer selbstständig geschafft hat, gültige Aktionen zu wählen, welche nicht zum Abbruch des Spiels geführt haben. Das Vorgehen, bei dem das Spiel nach Wahl einer ungültigen Aktion abgebrochen wurde, ist nach einigen Tagen verworfen worden, da es der Künstlichen Intelligenz das Lernen erschwert hat, da die meisten Spiele bereits nach wenigen Schritten endeten und die Künstliche Intelligenz so wenig relevante Daten sammeln konnte. Außerdem war die Bewertung der Performanz ebenfalls, durch die vielen Spielabbrüche, erschwert. Daraufhin wurde zunächst ein Verfahren implementiert, bei dem das Spiel bei ungültigen Aktionen nicht sofort abgebrochen, sondern negativ Belohnt worden ist. Diese Vorgehen stellte sich als recht tauglich heraus, wurde aber schnell von einem Vorgehen, welches eine Aktionsmaske verwendet abgelöst. Im folgenden wurde eine Aktionsmaske implementiert, welche gewährleistet, dass nur gültige Aktionen gewählt werden können. Somit musste die Künstliche Intelligenz nicht mehr selbstständig lernen, welche Aktionen gültig sind. Die Performanz stieg mit diesem Vorgehen von vorher durchschnittlich ungefähr 60 Prozent der Maximalpunktzahl auf 75 Prozent.

Zudem ergaben sich Schwierigkeiten bei der Erweiterung des Runden-Systems. Fast die gesamte Spielumgebung musste, an das neue Runden-System, angepasst werden. Dieser Aufwand wäre voraussichtlich deutlich geringer ausgefallen, wenn das Runden-System von Anfang an implementiert worden wäre.

Das finale Training der Künstlichen Intelligenz erfolgte innerhalb von drei Trainingsphasen. Zunächst wurde die KI in 2220000 Trainingsschritten vortrainiert. Anschließend wurde sie in 1110000 Trainingsschritten, mit einem verringerten Entropie Koeffizienten, was einer geringeren Exploration entspricht, nachtrainiert, um das gelernte Verhalten zu exploiden. Dieses Training erfolgte daraufhin mit der doppelten Zeit weitere zwei male. Das Ergebnis des Trainings war eine Künstliche Intelligenz, welche durchschnittlich 207 Punkte im Spiel erzielt. Im Vergleich dazu erzielten menschliche Spieler in einem Test lediglich 160 Punkte im Durchschnitt. Für die durchschnittlichen Punktezahlen, wurden die Erreichten Punkte einer abgeschlossenen Spielumgebung abgespeichert, anschließend aufaddiert und durch ihre Anzahl geteilt.
\end{abstract}

\newpage