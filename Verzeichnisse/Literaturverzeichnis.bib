
@inproceedings{jin_optimization_2022,
	title = {An Optimization Algorithm for the Game of {NoGo} based on {MCTS}},
	doi = {10.1109/GCRAIT55928.2022.00033},
	abstract = {The game of {NoGo} has been proposed in recent years. {NoGo} game and its related rules are firstly introduce. Then we discuss the problems exposed by the traditional {MCTS} algorithm at the beginning of the game, proposes an idea of creating a converse digraph and solves the shortcomings of the traditional {MCTS} (Monte-Carlo Tree Search) algorithm at the beginning of the game. Finally, the {NoGo} result between the proposed algorithm by this paper and other traditional algorithm is obtained. The experimental result show that our proposed algorithm are practical for {NoGo}.},
	eventtitle = {2022 Global Conference on Robotics, Artificial Intelligence and Information Technology ({GCRAIT})},
	pages = {120--122},
	booktitle = {2022 Global Conference on Robotics, Artificial Intelligence and Information Technology ({GCRAIT})},
	author = {Jin, Chenghou and Gao, Ming and Han, Yusen and Ma, Salimu},
	date = {2022-07},
	keywords = {Algorithms, Artificial intelligence, Converse digraph, Games, Information technology, {MCTS}, Monte Carlo methods, Neural networks, {NoGo} Game, Optimization, Search problems},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sande\\Zotero\\storage\\6RNN8IJJ\\stamp.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\MGYHZIRW\\Jin et al. - 2022 - An Optimization Algorithm for the Game of NoGo bas.pdf:application/pdf},
}

@article{scheiermann_alphazero-inspired_2022,
	title = {{AlphaZero}-Inspired Game Learning: Faster Training by Using {MCTS} Only at Test Time},
	issn = {2475-1510},
	doi = {10.1109/TG.2022.3206733},
	shorttitle = {{AlphaZero}-Inspired Game Learning},
	abstract = {Recently, the seminal algorithms {AlphaGo} and {AlphaZero} have started a new era in game learning and deep reinforcement learning. While the achievements of {AlphaGo} and {AlphaZero} – playing Go and other complex games at super human level – are truly impressive, these architectures have the drawback that they require high computational resources. Many researchers are looking for methods that are similar to {AlphaZero}, but have lower computational demands and are thus more easily reproducible. In this paper, we pick an important element of {AlphaZero} – the Monte Carlo Tree Search ({MCTS}) planning stage – and combine it with temporal difference ({TD}) learning agents. We wrap {MCTS} for the first time around {TD} n-tuple networks and we use this wrapping only at test time to create versatile agents that keep at the same time the computational demands low. We apply this new architecture to several complex games (Othello, {ConnectFour}, Rubik's Cube) and show the advantages achieved with this {AlphaZero}-inspired {MCTS} wrapper. In particular, we present results that this agent is the first one trained on standard hardware (no {GPU} or {TPU}) to beat the very strong Othello program Edax up to and including level 7 (where most other learning-from-scratch algorithms could only defeat Edax up to level 2).},
	pages = {1--11},
	journaltitle = {{IEEE} Transactions on Games},
	author = {Scheiermann, Johannes and Konen, Wolfgang},
	date = {2022},
	note = {Conference Name: {IEEE} Transactions on Games},
	keywords = {Games, Planning, Prediction algorithms, Silver, Task analysis, Training, Wrapping},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sande\\Zotero\\storage\\EGBLAPLS\\stamp.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\WZR4KK4K\\Scheiermann und Konen - 2022 - AlphaZero-Inspired Game Learning Faster Training .pdf:application/pdf},
}

@inproceedings{goodman_multitree_2022,
	title = {{MultiTree} {MCTS} in Tabletop Games},
	doi = {10.1109/CoG51982.2022.9893605},
	abstract = {We introduce {MultiTree} Monte Carlo Tree Search ({MT}-{MCTS}), in which a tree is constructed independently for each player. This permits deeper search for the acting agent’s own move, at the cost of a poorer opponent model and the loss of conditioning a move on the specific action of another player.We test {MT}-{MCTS} in eleven different tabletop board and card games, with varying numbers of players. The main benefit occurs in simultaneous-move games, where independent trees better model the information structure. We find that in other games {MT}-{MCTS} can outperform vanilla {MCTS}, which incorporates all players in a single tree, but that this advantage usually decreases as the computational budget increases, and the cost of poor opponent modelling outweighs the gain from deeper search.},
	eventtitle = {2022 {IEEE} Conference on Games ({CoG})},
	pages = {292--299},
	booktitle = {2022 {IEEE} Conference on Games ({CoG})},
	author = {Goodman, James and Perez-Liebana, Diego and Lucas, Simon},
	date = {2022-08},
	note = {{ISSN}: 2325-4289},
	keywords = {Games, {MCTS}, Monte Carlo methods, Planning, Computational modeling, Costs, Multiplayer, Opponent Modelling, Standards, Tabletop Games, Tuning},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sande\\Zotero\\storage\\LEGJR7UZ\\stamp.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\EZPEDVEC\\Goodman et al. - 2022 - MultiTree MCTS in Tabletop Games.pdf:application/pdf},
}

@article{litjens_mcts-based_nodate,
	title = {An {MCTS}-based {AI} playing The Crew},
	author = {Litjens, Iwan},
	langid = {english},
	file = {Litjens - An MCTS-based AI playing The Crew.pdf:C\:\\Users\\sande\\Zotero\\storage\\JWW9S24S\\Litjens - An MCTS-based AI playing The Crew.pdf:application/pdf},
}

@inproceedings{swiechowski_combining_2021,
	title = {Combining Utility {AI} and {MCTS} Towards Creating Intelligent Agents in Video Games, with the Use Case of Tactical Troops: Anthracite Shift},
	doi = {10.1109/SSCI50451.2021.9660170},
	shorttitle = {Combining Utility {AI} and {MCTS} Towards Creating Intelligent Agents in Video Games, with the Use Case of Tactical Troops},
	abstract = {Tactical Troops: Anthracite Shift is a squad-based tactics game with complex environment. It is a commercial game released on the Steam platform. This paper is authored by its original developers. It presents how the {AI} -driven players, that are featured in the game, have been designed and implemented. Their procedure of operation is based on a hierarchical combination of Monte Carlo Tree Search and Utility {AI}. Although these methods alone have already been applied to video games, there have not been published works that employ such a combination. Solutions to creating the {AI} in commercial games are rarely open to the public, which motivated us even more. The quality of the {AI} agents has been assessed from two perspectives: playing strength and believability.},
	eventtitle = {2021 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
	pages = {1--8},
	booktitle = {2021 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
	author = {Świechowski, Maciej and Lewiński, Daniel and Tyl, Rafal},
	date = {2021-12},
	keywords = {Games, Monte Carlo methods, Complexity theory, computer bots, Control systems, Decision making, Industries, Intelligent agents, Monte Carlo Tree Search, simulation, utility theory, video game},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sande\\Zotero\\storage\\JQ738KGP\\stamp.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\USPGPVXG\\Świechowski et al. - 2021 - Combining Utility AI and MCTS Towards Creating Int.pdf:application/pdf},
}

@article{kartal_action_2019,
	title = {Action Guidance with {MCTS} for Deep Reinforcement Learning},
	volume = {15},
	rights = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0924},
	url = {https://ojs.aaai.org/index.php/AIIDE/article/view/5238},
	doi = {10.1609/aiide.v15i1.5238},
	abstract = {Deep reinforcement learning has achieved great successes in recent years, however, one main challenge is the sample inefficiency. In this paper, we focus on how to use action guidance by means of a non-expert demonstrator to improve sample efficiency in a domain with sparse, delayed, and possibly deceptive rewards: the recently-proposed multi-agent benchmark of Pommerman. We propose a new framework where even a non-expert simulated demonstrator, e.g., planning algorithms such as Monte Carlo tree search with a small number rollouts, can be integrated within asynchronous distributed deep reinforcement learning methods. Compared to a vanilla deep {RL} algorithm, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.},
	pages = {153--159},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence and Interactive Digital Entertainment},
	author = {Kartal, Bilal and Hernandez-Leal, Pablo and Taylor, Matthew E.},
	urldate = {2023-07-04},
	date = {2019-10-08},
	langid = {english},
	note = {Number: 1},
	file = {Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\V8PTP8YF\\Kartal et al. - 2019 - Action Guidance with MCTS for Deep Reinforcement L.pdf:application/pdf},
}
