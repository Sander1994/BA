
@inproceedings{jin_optimization_2022,
	title = {An Optimization Algorithm for the Game of {NoGo} based on {MCTS}},
	doi = {10.1109/GCRAIT55928.2022.00033},
	abstract = {The game of {NoGo} has been proposed in recent years. {NoGo} game and its related rules are firstly introduce. Then we discuss the problems exposed by the traditional {MCTS} algorithm at the beginning of the game, proposes an idea of creating a converse digraph and solves the shortcomings of the traditional {MCTS} (Monte-Carlo Tree Search) algorithm at the beginning of the game. Finally, the {NoGo} result between the proposed algorithm by this paper and other traditional algorithm is obtained. The experimental result show that our proposed algorithm are practical for {NoGo}.},
	eventtitle = {2022 Global Conference on Robotics, Artificial Intelligence and Information Technology ({GCRAIT})},
	pages = {120--122},
	booktitle = {2022 Global Conference on Robotics, Artificial Intelligence and Information Technology ({GCRAIT})},
	author = {Jin, Chenghou and Gao, Ming and Han, Yusen and Ma, Salimu},
	date = {2022-07},
	keywords = {Algorithms, Artificial intelligence, Converse digraph, Games, Information technology, {MCTS}, Monte Carlo methods, Neural networks, {NoGo} Game, Optimization, Search problems},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sande\\Zotero\\storage\\6RNN8IJJ\\stamp.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\MGYHZIRW\\Jin et al. - 2022 - An Optimization Algorithm for the Game of NoGo bas.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	number = {{arXiv}:1707.06347},
	publisher = {{arXiv}},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2023-07-23},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\sande\\Zotero\\storage\\NUDDJS45\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sande\\Zotero\\storage\\X3CRBW5G\\1707.html:text/html},
}

@incollection{alpaydin_maschinelles_2022,
	title = {Maschinelles Lernen},
	rights = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	isbn = {978-3-11-074019-6},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110740196/html},
	abstract = {Maschinelles Lernen ist die künstliche Generierung von Wissen aus Erfahrung. Dieses Buch diskutiert Methoden aus den Bereichen Statistik, Mustererkennung und kombiniert die unterschiedlichen Ansätze, um effiziente Lösungen zu finden. Diese Auflage bietet ein neues Kapitel über Deep Learning und erweitert die Inhalte über mehrlagige Perzeptrone und bestärkendes Lernen. Eine neue Sektion über erzeugende gegnerische Netzwerke ist ebenfalls dabei.},
	booktitle = {Maschinelles Lernen},
	publisher = {De Gruyter Oldenbourg},
	author = {Alpaydin, Ethem},
	urldate = {2023-10-25},
	date = {2022-01-19},
	langid = {german},
	doi = {10.1515/9783110740196},
	keywords = {Deep Learning, Künstliche Intelligenz, Maschinelles Lernen, Neuronale Netze},
	file = {Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\KWBGQVY6\\Alpaydin - 2022 - Maschinelles Lernen.pdf:application/pdf},
}

@book{ris-ala_fundamentals_2023,
	location = {Cham},
	title = {Fundamentals of Reinforcement Learning},
	isbn = {978-3-031-37344-2 978-3-031-37345-9},
	url = {https://link.springer.com/10.1007/978-3-031-37345-9},
	publisher = {Springer Nature Switzerland},
	author = {Ris-Ala, Rafael},
	urldate = {2023-10-25},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-3-031-37345-9},
	keywords = {Deep Learning, Artificial Intelligence, Machine Learning, Markov Chain, Q-Learning Algorithm},
	file = {Full Text PDF:C\:\\Users\\sande\\Zotero\\storage\\DLYCHXI5\\Ris-Ala - 2023 - Fundamentals of Reinforcement Learning.pdf:application/pdf},
}

@book{sewak_deep_2019,
	location = {Singapore, {SINGAPORE}},
	title = {Deep Reinforcement Learning: Frontiers of Artificial Intelligence},
	isbn = {9789811382857},
	url = {http://ebookcentral.proquest.com/lib/hs-coburg/detail.action?docID=5802517},
	shorttitle = {Deep Reinforcement Learning},
	abstract = {This book starts by presenting the basics of reinforcement learning using highly intuitive and easy-to-understand examples and applications, and then introduces the cutting-edge research advances that make reinforcement learning capable of out-performing most state-of-art systems, and even humans in a number of applications. The book not only equips readers with an understanding of multiple advanced and innovative algorithms, but also prepares them to implement systems such as those created by Google Deep Mind in actual code. This book is intended for readers who want to both understand and apply advanced concepts in a field that combines the best of two worlds - deep learning and reinforcement learning - to tap the potential of 'advanced artificial intelligence' for creating real-world applications and game-winning algorithms.},
	publisher = {Springer Singapore Pte. Limited},
	author = {Sewak, Mohit},
	urldate = {2023-10-25},
	date = {2019},
	keywords = {Reinforcement learning.},
	file = {ProQuest Ebook Snapshot:C\:\\Users\\sande\\Zotero\\storage\\AYQ9I97E\\detail.html:text/html},
}

@online{noauthor_gymnasium_nodate,
	title = {Gymnasium Documentation},
	url = {https://gymnasium.farama.org/index.html},
	abstract = {A standard {API} for reinforcement learning and a diverse set of reference environments (formerly Gym)},
	urldate = {2023-10-27},
	langid = {english},
	file = {Snapshot:C\:\\Users\\sande\\Zotero\\storage\\K8MFMXLE\\gymnasium.farama.org.html:text/html},
}

@online{noauthor_stable-baselines3_nodate,
	title = {Stable-Baselines3 Docs - Reliable Reinforcement Learning Implementations — Stable Baselines3 2.2.0a8 documentation},
	url = {https://stable-baselines3.readthedocs.io/en/master/index.html},
	urldate = {2023-10-27},
	file = {Stable-Baselines3 Docs - Reliable Reinforcement Learning Implementations — Stable Baselines3 2.2.0a8 documentation:C\:\\Users\\sande\\Zotero\\storage\\SN54B3KB\\index.html:text/html},
}

@online{noauthor_matplotlib_nodate,
	title = {Matplotlib — Visualization with Python},
	url = {https://matplotlib.org/},
	urldate = {2023-10-27},
	file = {Matplotlib — Visualization with Python:C\:\\Users\\sande\\Zotero\\storage\\ZBSAUPJF\\matplotlib.org.html:text/html},
}

@article{ran_stable-baselines3_nodate,
	title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
	abstract = {Stable-Baselines3 provides open-source implementations of deep reinforcement learning ({RL}) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95\% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare diﬀerent {RL} algorithms. Our documentation, examples, and source-code are available at https://github.com/{DLR}-{RM}/stable-baselines3.},
	author = {Raﬃn, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
	langid = {english},
	file = {Raﬃn et al. - Stable-Baselines3 Reliable Reinforcement Learning.pdf:C\:\\Users\\sande\\Zotero\\storage\\VCVVJFF8\\Raﬃn et al. - Stable-Baselines3 Reliable Reinforcement Learning.pdf:application/pdf},
}

@online{noauthor_matplotlib_nodate-1,
	title = {Matplotlib — Visualization with Python},
	url = {https://matplotlib.org/},
	urldate = {2023-10-29},
	file = {Matplotlib — Visualization with Python:C\:\\Users\\sande\\Zotero\\storage\\SLRQVD9U\\matplotlib.org.html:text/html},
}

@online{noauthor_ganz_2022,
	title = {Ganz schön clever - Test, Bewertung, Regeln \& Strategie},
	url = {https://spielenerds.de/ganz-schoen-clever/},
	abstract = {Das Würfelspiel „Ganz schön clever“ erklärt und von uns bewertet: Zubehör und Aufbau, sowie Spielregeln inklusive unserer Strategie.},
	urldate = {2023-10-30},
	date = {2022-06-14},
	langid = {german},
	note = {Section: Brettspiel-Tests},
	file = {Snapshot:C\:\\Users\\sande\\Zotero\\storage\\G5QN9PUZ\\ganz-schoen-clever.html:text/html},
}
